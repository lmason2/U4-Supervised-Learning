{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Naive Bayes\n",
    "What are our learning objectives for this lesson?\n",
    "* Learn about Bayes Theorem\n",
    "* Learn about the Naive Bayes classification algorithm\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s)\n",
    "TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today \n",
    "TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayesian Classification\n",
    "Basic ideas\n",
    "* Predict class labels based on probabilities (statistics)\n",
    "* Naive Bayes comparable in performance to \"fancier\" approaches\n",
    "* Relatively efficient on large datasets\n",
    "* Assumes \"conditional independence\"\n",
    "    * Effect of one attribute on a class is independent from other attributes\n",
    "    * Why it is called \"naive\"\n",
    "    * Helps with execution time (speed)\n",
    "\n",
    "## Basic Probability\n",
    "$P(H)$ ... the probability of event $H$\n",
    "* $H$ (hypothesis) for us would be that any given instance is of a class $C$\n",
    "* Called the prior probability\n",
    "\n",
    "$P(X)$ ... the probability of event $X$\n",
    "* For us, $X$ would be an instance (a row in a table)\n",
    "* The probability that an instance would have $X$'s attribute values\n",
    "* Also a prior probability\n",
    "\n",
    "$P(X|H)$ ... the conditional probability of $X$ given $H$\n",
    "* The probability of Xâ€™s attribute values assuming we know it is of class C\n",
    "* Called the posterior probability\n",
    "\n",
    "$P(H|X)$ ... the conditional probability of $H$ given $X$\n",
    "* The probability that $X$ is of class $C$ given $X$'s attribute values\n",
    "* Also a posterior probability\n",
    "* This is the one we want to know to make predictions!\n",
    "    * i.e., we want the $C$ that gives the highest probability\n",
    "* We can estimate $P(H)$, $P(X)$, and $P(X|H)$ from the training set\n",
    "* From these, we can use Bayes Theorem to estimate $P(H|X)$:\n",
    "\n",
    "Bayes Theorem:\n",
    "$$P(H|X) = \\frac{P(X|H)P(H)}{P(X)}$$\n",
    "\n",
    "Basic idea behind Bayes Theorem:\n",
    "If $P(A \\cap B)$ is the probability that both $A$ and $B$ occur, then:\n",
    "$$P(A \\cap B) = P(A|B)P(B) = P(B|A)P(A)$$\n",
    "\n",
    "In other words:\n",
    "* Let's say $A$ occurs $x$% of the time given (within) $B$\n",
    "* And $B$ occurs $y$% of the time\n",
    "* Then $A$ and $B$ occur together, i.e., $A \\cap B$: $x$% $\\cdot y$% of the time\n",
    "\n",
    "\n",
    "For example:\n",
    "* Assume we have a bucket of Lego bricks\n",
    "* 50% of the 1x2 bricks are Red\n",
    "* 10% of the bricks in the bucket are 1x2's\n",
    "* Then, 50% of the 10% of 1x2's are Red-1x2's (i.e., 50% $\\cdot$ 10%)\n",
    "\n",
    "We can use the equality to derive Bayes Theorem:\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "## Classification Approach\n",
    "Basic Approach:\n",
    "1. We're given an instance $X = [v_1, v_2, ..., v_n]$ to classify\n",
    "1. For each class $C_1, C_2, ... , C_m$, we want to find the class $C_i$ such that:\n",
    "$$P(C_i|X) > P(C_j|X) \\: \\textrm{for} \\: i \\leq j \\leq m, j \\neq i$$\n",
    "In other words, we want to find the class $C_i$ with the largest $P(C_i|X)$\n",
    "1. Use Bayes Theorem to find each $P(C|X)$, i.e., for each $C_i$ calculate:\n",
    "$$P(C_i|X) = P(X|C_i)P(C_i)$$\n",
    "We leave out $P(X)$ since it is the same for all classes ($C_i$'s)\n",
    "1. We estimate $P(C)$ as the percentage of $C$-labeled rows in the training set\n",
    "$$P(C) = \\frac{|C|}{D}$$\n",
    "where $|C|$ is the number of instances classified as $C$ in the training set and $D$ is the training set size\n",
    "1. We estimate $P(X|C)$ using the independence assumption of attributes:\n",
    "$$P(X|C) = \\prod_{k=1}^{n}P(v_k|C)$$\n",
    "If attribute $k$ is categorical\n",
    "    * We estimate $P(v_k|C)$ as the percentage of instances with value $v_k$ (in attribute $k$) across training set instances of class $C$\n",
    "    \n",
    "Some notes:\n",
    "* Step 5 is an optimization: comparing entire rows is expensive (esp. if many attributes)\n",
    "* For smaller datasets, there may also not be any matches\n",
    "* Can extend the approach to support continuous attributes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Tasks\n",
    "### Lab Task 1\n",
    "Consider the following labeled dataset, where result denotes class information and the remaining columns have categorical values.\n",
    "\n",
    "|att1|att2|result|\n",
    "|-|-|-|\n",
    "|1|5|yes|\n",
    "|2|6|yes|\n",
    "|1|5|no|\n",
    "|1|5|no|\n",
    "|1|6|yes|\n",
    "|2|6|no|\n",
    "|1|5|yes|\n",
    "|1|6|yes|\n",
    "\n",
    "1. Compute the priors for the dataset (e.g. what is $P(result = yes)$ and $P (result = no)$?)\n",
    "1. Compute the posteriors (conditional probabilities) for the dataset by making a table like Bramer 3.2 (e.g. what is $P(att1 = 1|result = yes)$, $P(att1 = 2|result = yes)$, $P(att2 = 5|result = yes)$, ...\n",
    "1. If $X = [1, 5]$, what is $P(result = yes|X)$ and $P(result = no|X)$ assuming conditional independence? Show your work.\n",
    "    1. What would the class label prediction be for the instance $X = [1, 5]$? Show your work.\n",
    "1. If $X = [1, 5]$, what is $P(result = yes|X)$ and $P(result = no|X)$ *without* assuming conditional independence? Show your work.\n",
    "    1. What would the class label prediction be for the instance $X = [1, 5]$? Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 2\n",
    "Example adapted from [this Naive Bayes example](https://www.geeksforgeeks.org/naive-bayes-classifiers/)\n",
    "\n",
    "Suppose we have the following dataset that has four attributes and a class attribute (PLAY GOLF):\n",
    "\n",
    "|OUTLOOK\t|TEMPERATURE\t|HUMIDITY\t|WINDY\t|PLAY GOLF|\n",
    "|-|-|-|-|-|\n",
    "|Rainy\t|Hot\t|High\t|False\t|No|\n",
    "|Rainy\t|Hot\t|High\t|True\t|No|\n",
    "|Overcast\t|Hot\t|High\t|False\t|Yes|\n",
    "|Sunny\t|Mild\t|High\t|False\t|Yes|\n",
    "|Sunny\t|Cool\t|Normal\t|False\t|Yes|\n",
    "|Sunny\t|Cool\t|Normal\t|True\t|No|\n",
    "|Overcast\t|Cool\t|Normal\t|True\t|Yes|\n",
    "|Rainy\t|Mild\t|High\t|False\t|No|\n",
    "|Rainy\t|Cool\t|Normal\t|False\t|Yes|\n",
    "|Sunny\t|Mild\t|Normal\t|False\t|Yes|\n",
    "|Rainy\t|Mild\t|Normal\t|True\t|Yes|\n",
    "|Overcast\t|Mild\t|High\t|True\t|Yes|\n",
    "|Overcast\t|Hot\t|Normal\t|False\t|Yes|\n",
    "|Sunny\t|Mild\t|High\t|True\t|No|\n",
    "\n",
    "Suppose we have a new instance X = \\[Sunny, Hot, Normal, False\\]. \n",
    "1. What is $P(PLAY GOLF = Yes|X)$? \n",
    "1. What is $P(PLAY GOLF = No|X)$? \n",
    "1. What is the prediction for X?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Continuous Data\n",
    "Assume attribute k is a continuous attribute sampled from a Gaussian distribution. We want to use attribute k with Naive Bayes. We will need to be able to compute $P(v_k|C)$ for a value $v_k$\n",
    "* Let $\\mu_C$ be the mean of attribute $k$ for instances labeled as $C$\n",
    "* Let $\\sigma_C$ be the standard deviation of attribute $k$ for instances labeled as $C$\n",
    "* The probability $P(v_k|C)$ is defined as:\n",
    "$$P(v_k|C) = g(v_k, \\mu_C, \\sigma_C)$$\n",
    "where the Gaussian function $g$ is defined as:\n",
    "$$g(x, \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "* This looks pretty messy, but it is relatively straightforward in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def gaussian(x, mean, sdev):\n",
    "    first, second = 0, 0\n",
    "    if sdev > 0:\n",
    "        first = 1 / (math.sqrt(2 * math.pi) * sdev)\n",
    "        second = math.e ** (-((x - mean) ** 2) / (2 * (sdev ** 2)))\n",
    "    return first * second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task 3\n",
    "Open ClassificationFun/main.py. Let's assume both `att1` `att2` are continuous and sampled from a normal (gaussian) distribution. \n",
    "1. What is $P(att1 = 2 | result = yes)$?\n",
    "1. What is $P(att2 = 3 | result = yes)$?\n",
    "1. What is $P(result = yes | X = [2, 3])$?\n",
    "1. What is $P(result = no | X = [2, 3])$?\n",
    "1. What is Naive Bayes' prediction for X = [2, 3]?\n",
    "1. How does this compare to kNN's prediction for X?\n",
    "\n",
    "What is $P(att1 = 2 | result = yes)$? First looking at class C = \"yes\", we need to compute the average and standard deviation of `att1` where C = \"yes\". Then we can compute the posterior $P(att1 = 2 | result = yes)$ using the `gaussian()` function and `att1=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 1, 0, 1]\n",
      "0.17010955993225252\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train = [\n",
    "    [3, 2],\n",
    "    [6, 6],\n",
    "    [4, 1],\n",
    "    [4, 4],\n",
    "    [1, 2],\n",
    "    [2, 0],\n",
    "    [0, 3],\n",
    "    [1, 6]\n",
    "]\n",
    "train_labels = [\"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"yes\"]\n",
    "test = [2, 3]\n",
    "\n",
    "att1_class_yes = []\n",
    "for i, row in enumerate(train):\n",
    "    if train_labels[i] == \"yes\":\n",
    "        att1_class_yes.append(row[0])\n",
    "print(att1_class_yes)\n",
    "mean = np.mean(att1_class_yes)\n",
    "stdev = np.std(att1_class_yes)\n",
    "# note: v_k (x is the formula) is test[0] att1 = 2\n",
    "p_att1_2_given_class_yes = gaussian(test[0], mean, stdev)\n",
    "print(p_att1_2_given_class_yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is $P(att2 = 3 | result = yes)$? Repeat the above for `att2` where C = \"yes\" and `att2=3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 2, 3, 6]\n",
      "0.17488003967875107\n"
     ]
    }
   ],
   "source": [
    "att2_class_yes = []\n",
    "for i, row in enumerate(train):\n",
    "    if train_labels[i] == \"yes\":\n",
    "        att2_class_yes.append(row[1])\n",
    "print(att2_class_yes)\n",
    "mean = np.mean(att2_class_yes)\n",
    "stdev = np.std(att2_class_yes)\n",
    "# note: v_k (x is the formula) is test[1] att2 = 3\n",
    "p_att2_3_given_class_yes = gaussian(test[1], mean, stdev)\n",
    "print(p_att2_3_given_class_yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is $P(result = yes | X = [2, 3])$? $P(result = yes | X = [2, 3]) = P(att1 = 2 | result = yes) \\times P(att2 = 3 | result = yes) \\times P(result = yes)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014874383295343602\n"
     ]
    }
   ],
   "source": [
    "p_yes_given_test = p_att1_2_given_class_yes * p_att2_3_given_class_yes * (4 / 8)\n",
    "print(p_yes_given_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try finishing the rest of the lab task questions from here :)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
