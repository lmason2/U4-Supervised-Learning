{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Naive Bayes\n",
    "What are our learning objectives for this lesson?\n",
    "* Learn about Bayes Theorem\n",
    "* Learn about the Naive Bayes classification algorithm\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayesian Classification\n",
    "Basic ideas\n",
    "* Predict class labels based on probabilities (statistics)\n",
    "* Naive Bayes comparable in performance to \"fancier\" approaches\n",
    "* Relatively efficient on large datasets\n",
    "* Assumes \"conditional independence\"\n",
    "    * Effect of one attribute on a class is independent from other attributes\n",
    "    * Why it is called \"naive\"\n",
    "    * Helps with execution time (speed)\n",
    "\n",
    "## Basic Probability\n",
    "$P(H)$ ... the probability of event $H$\n",
    "* $H$ (hypothesis) for us would be that any given instance is of a class $C$\n",
    "* Called the prior probability\n",
    "\n",
    "$P(X)$ ... the probability of event $X$\n",
    "* For us, $X$ would be an instance (a row in a table)\n",
    "* The probability that an instance would have $X$'s attribute values\n",
    "* Also a prior probability\n",
    "\n",
    "$P(X|H)$ ... the conditional probability of $X$ given $H$\n",
    "* The probability of Xâ€™s attribute values assuming we know it is of class C\n",
    "* Called the posterior probability\n",
    "\n",
    "$P(H|X)$ ... the conditional probability of $H$ given $X$\n",
    "* The probability that $X$ is of class $C$ given $X$'s attribute values\n",
    "* Also a posterior probability\n",
    "* This is the one we want to know to make predictions!\n",
    "    * i.e., we want the $C$ that gives the highest probability\n",
    "* We can estimate $P(H)$, $P(X)$, and $P(X|H)$ from the training set\n",
    "* From these, we can use Bayes Theorem to estimate $P(H|X)$:\n",
    "\n",
    "Bayes Theorem:\n",
    "$$P(H|X) = \\frac{P(X|H)P(H)}{P(X)}$$\n",
    "\n",
    "Basic idea behind Bayes Theorem:\n",
    "If $P(A \\cap B)$ is the probability that both $A$ and $B$ occur, then:\n",
    "$$P(A \\cap B) = P(A|B)P(B) = P(B|A)P(A)$$\n",
    "\n",
    "In other words:\n",
    "* Let's say $A$ occurs $x$% of the time given (within) $B$\n",
    "* And $B$ occurs $y$% of the time\n",
    "* Then $A$ and $B$ occur together, i.e., $A \\cap B$: $x$% $\\cdot y$% of the time\n",
    "\n",
    "\n",
    "For example:\n",
    "* Assume we have a bucket of Lego bricks\n",
    "* 50% of the 1x2 bricks are Red\n",
    "* 10% of the bricks in the bucket are 1x2's\n",
    "* Then, 50% of the 10% of 1x2's are Red-1x2's (i.e., 50% $\\cdot$ 10%)\n",
    "\n",
    "We can use the equality to derive Bayes Theorem:\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "## Classification Approach\n",
    "Basic Approach:\n",
    "1. We're given an instance $X = [v_1, v_2, ..., v_n]$ to classify\n",
    "1. For each class $C_1, C_2, ... , C_m$, we want to find the class $C_i$ such that:\n",
    "$$P(C_i|X) > P(C_j|X) \\: \\textrm{for} \\: i \\leq j \\leq m, j \\neq i$$\n",
    "In other words, we want to find the class $C_i$ with the largest $P(C_i|X)$\n",
    "1. Use Bayes Theorem to find each $P(C|X)$, i.e., for each $C_i$ calculate:\n",
    "$$P(C_i|X) = P(X|C_i)P(C_i)$$\n",
    "We leave out $P(X)$ since it is the same for all classes ($C_i$'s)\n",
    "1. We estimate $P(C)$ as the percentage of $C$-labeled rows in the training set\n",
    "$$P(C) = \\frac{|C|}{D}$$\n",
    "where $|C|$ is the number of instances classified as $C$ in the training set and $D$ is the training set size\n",
    "1. We estimate $P(X|C)$ using the independence assumption of attributes:\n",
    "$$P(X|C) = \\prod_{k=1}^{n}P(v_k|C)$$\n",
    "If attribute $k$ is categorical\n",
    "    * We estimate $P(v_k|C)$ as the percentage of instances with value $v_k$ (in attribute $k$) across training set instances of class $C$\n",
    "    \n",
    "Some notes:\n",
    "* Step 5 is an optimization: comparing entire rows is expensive (esp. if many attributes)\n",
    "* For smaller datasets, there may also not be any matches\n",
    "* Can extend the approach to support continuous attributes...\n",
    "\n",
    "## Lab Tasks\n",
    "### Lab Task 1\n",
    "Consider the following labeled dataset, where result denotes class information and the remaining columns have categorical values.\n",
    "\n",
    "|att1|att2|result|\n",
    "|-|-|-|\n",
    "|1|5|yes|\n",
    "|2|6|yes|\n",
    "|1|5|no|\n",
    "|1|5|no|\n",
    "|1|6|yes|\n",
    "|2|6|no|\n",
    "|1|5|yes|\n",
    "|1|6|yes|\n",
    "\n",
    "1. What is $P(result = yes)$ and $P (result = no)$ given the dataset?\n",
    "1. If $X = [1,5]$, what is $P(X|result = yes)$ and $P(X|result = no)$ if we assume conditional independence? Show your work.\n",
    "1. Using Naive Bayes with conditional independence, what would the class label prediction be for the instance $X = [1, 5]$? Show your work.\n",
    "\n",
    "### Lab Task 2\n",
    "iPhone Purchases (Fake)\n",
    "\n",
    "|standing |job_status |credit_rating |buys_iphone|\n",
    "|-|-|-|-|\n",
    "|1 |3 |fair |no|\n",
    "|1 |3 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|2 |1 |fair |yes|\n",
    "|2 |1 |excellent |no|\n",
    "|2 |1 |excellent |yes|\n",
    "|1 |2 |fair |no|\n",
    "|1 |1 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|1 |2 |excellent |yes|\n",
    "|2 |2 |excellent |yes|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "\n",
    "* buys_iphone is the class (with labels \"yes\" and \"no\")\n",
    "* standing: 1 = lower division, 2 = upper division\n",
    "* job status: 1 = none, 2 = < 20 hrs, 3 = $\\geq$ 20 hrs\n",
    "\n",
    "Tasks TBA..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3/4 Warm-up Task(s)\n",
    "Example adapted from [this Naive Bayes example](https://www.geeksforgeeks.org/naive-bayes-classifiers/)\n",
    "\n",
    "Suppose we have the following dataset that has four attributes and a class attribute (PLAY GOLF):\n",
    "\n",
    "|OUTLOOK\t|TEMPERATURE\t|HUMIDITY\t|WINDY\t|PLAY GOLF|\n",
    "|-|-|-|-|-|\n",
    "|Rainy\t|Hot\t|High\t|False\t|No|\n",
    "|Rainy\t|Hot\t|High\t|True\t|No|\n",
    "|Overcast\t|Hot\t|High\t|False\t|Yes|\n",
    "|Sunny\t|Mild\t|High\t|False\t|Yes|\n",
    "|Sunny\t|Cool\t|Normal\t|False\t|Yes|\n",
    "|Sunny\t|Cool\t|Normal\t|True\t|No|\n",
    "|Overcast\t|Cool\t|Normal\t|True\t|Yes|\n",
    "|Rainy\t|Mild\t|High\t|False\t|No|\n",
    "|Rainy\t|Cool\t|Normal\t|False\t|Yes|\n",
    "|Sunny\t|Mild\t|Normal\t|False\t|Yes|\n",
    "|Rainy\t|Mild\t|Normal\t|True\t|Yes|\n",
    "|Overcast\t|Mild\t|High\t|True\t|Yes|\n",
    "|Overcast\t|Hot\t|Normal\t|False\t|Yes|\n",
    "|Sunny\t|Mild\t|High\t|True\t|No|\n",
    "\n",
    "Suppose we have a new instance X = \\[Sunny, Hot, Normal, False\\]. \n",
    "1. What is $P(PLAY GOLF = Yes|X)$? \n",
    "1. What is $P(PLAY GOLF = No|X)$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3/6 Warm-up Task(s)\n",
    "Open ClassificationFun/main.py\n",
    "1. What is P(att1 = 2 | result = yes)?\n",
    "1. What is P(att2 = 3 | result = yes)?\n",
    "1. What is P(result = yes | X = \\[2, 3\\])?\n",
    "1. What is P(result = no | X = \\[2, 3\\])?\n",
    "1. What is Naive Bayes' prediction for X = \\[2, 3\\]?\n",
    "    1. How does this compare to kNN's prediction for X?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Continuous Data\n",
    "Assume attribute k is a continuous attribute sampled from a Gaussian distribution. We want to use attribute k with Naive Bayes. We will need to be able to compute $P(v_k|C)$ for a value $v_k$\n",
    "* Let $\\mu_C$ be the mean of attribute $k$ for instances labeled as $C$\n",
    "* Let $\\sigma_C$ be the standard deviation of attribute $k$ for instances labeled as $C$\n",
    "* The probability $P(v_k|C)$ is defined as:\n",
    "$$P(v_k|C) = g(v_k, \\mu_C, \\sigma_C)$$\n",
    "where the Gaussian function $g$ is defined as:\n",
    "$$g(x, \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "* This looks pretty messy, but it is relatively straightforward in Python:\n",
    "```python\n",
    "import math\n",
    "def gaussian(x, mean, sdev):\n",
    "    first, second = 0, 0\n",
    "    if sdev > 0:\n",
    "        first = 1 / (math.sqrt(2 * math.pi) * sdev)\n",
    "        second = math.e ** (-((x - mean) ** 2) / (2 * (sdev ** 2)))\n",
    "    return first * second\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
