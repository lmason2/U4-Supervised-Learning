{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Classifier Evaluation\n",
    "What are our learning objectives for this lesson?\n",
    "* Divide a dataset into training and testing sets using different approaches\n",
    "* Evaluate classifier performance using different metrics\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s) 3/2\n",
    "1. Please answer the morning question at http://PollEv.com/ginasprint096\n",
    "1. Open ClassificationFun/main.py, that's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today  3/2\n",
    "* Announcements\n",
    "    * Let's go over IQ4\n",
    "    * IQ5 on Thursday on data visualization and PA3 topics\n",
    "    * RQ5 is posted and due on Monday\n",
    "    * PA4 is posted and due next Wednesday, 3/10.\n",
    "* ClassificationFun\n",
    "* Classifier evaluation metrics\n",
    "* Go over PA4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s) 3/4\n",
    "In ClassificationFun/main.py:\n",
    "1. Write a function called `randomize_in_place(alist, parallel_list=None)` that accepts at least one list and shuffles the elements of the list. If a second list is passed in, it should be shuffled in the same order.\n",
    "1. Call your function with the `train` and `train_labels` (parallel lists) lists. Make sure they get shuffled in parallel :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today 3/4\n",
    "* Announcements\n",
    "    * Great senior design presentations yesterday!\n",
    "    * RQ5 due on **Tuesday** (so you can check your work after some examples of Naive Bayes we will do on Tuesday)\n",
    "    * PA4 due Wednesday\n",
    "    * PA5 will be out later today\n",
    "* Classifier evaluation metrics\n",
    "* Start of classification with Naive Bayes\n",
    "* IQ5 last ~15 mins of class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "Building a classifier starts with a learning (training) phase\n",
    "* Based on predefined set of examples (AKA the training set)\n",
    "\n",
    "The classifier is then evaluated for predictive accuracy (% of test instances correctly classified by the classifier)\n",
    "* Based on another set of examples (AKA the testing set)\n",
    "* We use the actual labels of the examples to test the predictions\n",
    "\n",
    "In general, we want to try to avoid overfitting\n",
    "* That is, encoding particular characteristics/anomalies of the training set into the classifier\n",
    "* Similar notion is \"underfitting\" (too simple of a model, e.g., linear instead of polynomial)\n",
    "\n",
    "We are going to discuss different ways to select training and testing sets\n",
    "1. The Holdout method\n",
    "2. Random Subsampling\n",
    "3. $k$-Fold Cross Validation and Variants\n",
    "4. Bootstrap Method\n",
    "\n",
    "### Holdout Method\n",
    "In the holdout method, the dataset is divided into two sets, the training and the testing set. The training set is used to build the model and the testing set is used to evaluate the model (e.g. the model's accuracy).\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/2000px-Supervised_machine_learning_in_a_nutshell.svg.png)\n",
    "(image from https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/2000px-Supervised_machine_learning_in_a_nutshell.svg.png)\n",
    "\n",
    "Approaches to the holdout method\n",
    "* Randomly divide data set into a training and test set\n",
    "* Partition evenly or, e.g., $\\frac{2}{3}$ to $\\frac{1}{3}$ (2:1) training to test set\n",
    "* This is random selection without replacement\n",
    "\n",
    "Q: Write a function to do a 2:1 partition in Python...\n",
    "```python\n",
    "import random\n",
    "\n",
    "def compute_holdout_partitions(table):\n",
    "    # randomize the table\n",
    "    randomized = table[:] # copy the table\n",
    "    n = len(table)\n",
    "    for i in range(n):\n",
    "        # pick an index to swap\n",
    "        j = random.randrange(0, n) # random int in [0,n) \n",
    "        randomized[i], randomized[j] = randomized[j], randomized[i]\n",
    "    # return train and test sets\n",
    "    split_index = int(2 / 3 * n) # 2/3 of randomized table is train, 1/3 is test\n",
    "    return randomized[0:split_index], randomized[split_index:]\n",
    "```\n",
    "\n",
    "### Random Subsampling Method\n",
    "* Repeat the holdout method $k$ times\n",
    "* Accuracy estimate is the average of the accuracy of each iteration\n",
    "\n",
    "### k-Fold Cross-Validation Method\n",
    "One of the shortcomings of the hold out method is the evaluation of the model depends heavily on which examples are selected for training versus testing. K-fold cross validation is a model evaluation approach that addresses this shortcoming of the holdout method.\n",
    "* Initial dataset partitioned into $k$ subsets (\"folds\") $D_1, D_2,..., D_k$\n",
    "* Each fold is approximately the same size\n",
    "* Training and testing is performed $k$ times:\n",
    "    * In iteration $i$, $D_i$ is used as the test set\n",
    "    * And $D_1 \\cup ... \\cup D_{iâˆ’1} \\cup D_{i+1} \\cup ... \\cup D_k$ used as training set\n",
    "* Note each subset is used exactly once for testing\n",
    "* Accuracy estimate is number of correct classifications over the $k$ iterations, divided by total number of rows (i.e., test instances) in the initial dataset\n",
    "* Alternatively, average accuracy by label\n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)\n",
    "\n",
    "\n",
    "(image from https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)\n",
    "\n",
    "### Variants of Cross-Validation\n",
    "Leave-one-out method\n",
    "* Special case of cross-validation where $k$ is the number of instances\n",
    "\n",
    "Stratified Cross-Validation method\n",
    "* Class distribution within folds is approximately the same as in the initial data\n",
    "\n",
    "Q: How might you go about generating stratified folds for cross validation?\n",
    "* One approach:\n",
    "    * Randomize the dataset\n",
    "    * Partition dataset so each subset contains rows with of a specific class\n",
    "        * e.g., if class label is \"yes\" or \"no\"\n",
    "        * Then one partition has all \"yes\" rows\n",
    "        * And the other all \"no\" rows\n",
    "        * Note: this is a group by class label\n",
    "    * Generate folds by:\n",
    "        * Iterating through each partition\n",
    "        * And distributing the partition (roughly) equally to each fold\n",
    "        \n",
    "### Lab Task 1\n",
    "Consider the following dataset:\n",
    "\n",
    "|att1|att2|result|\n",
    "|-|-|-|\n",
    "|3|2|no|\n",
    "|6|6|yes|\n",
    "|4|1|no|\n",
    "|4|4|no|\n",
    "|1|2|yes|\n",
    "|2|0|no|\n",
    "|0|3|yes|\n",
    "|1|6|yes|\n",
    "\n",
    "1. Assume we want to perform Stratified k-Fold Cross-Validation of our NN classifier for $k$ = 4. Create corresponding folds (partitions) for the dataset.\n",
    "2. Describe how these $k$ folds would be used to perform cross validation. That is, show how the $k$ test runs are performed.\n",
    "\n",
    "    \n",
    "### The Bootstrap Method\n",
    "* Like random subsampling but with replacement\n",
    "* Usually used for small datasets\n",
    "* The basic \".632\" approach:\n",
    "    * Given a dataset with $D$ rows\n",
    "    * Randomly select $D$ rows with replacement (i.e., might select same row)\n",
    "    * This gives a \"bootstrap sample\" (training set) of $D$ rows\n",
    "    * The remaining rows (not selected) form the test set\n",
    "    * On average, 63.2% of original rows will end up in the training set\n",
    "    * And 36.8% will end up in the test set\n",
    "* Why these percentages?\n",
    "    * Each row has a $1/D$ chance of being selected\n",
    "    * Each row has a (1 - 1/D) chance of not being selected\n",
    "    * We select $D$ times, so probability a row not chosen at all is $(1 - 1/D)^D$\n",
    "    * For large $D$, the probability approaches $e^{-1} = 0.368$ (for $e = 2.718$...)\n",
    "* The sampling procedure is repeated $k$ times\n",
    "    * Each iteration uses the test set for an accuracy estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classifier Performance\n",
    "Divide data set into a Training Set and a Test Set\n",
    "* \"Build\" classifier on training set\n",
    "* Test performance on the test set (try to predict their labels)\n",
    "* For the test set you know the \"ground truth\"\n",
    "\n",
    "Assume we have 2-valued class labels (e.g., \"yes\" and \"no\")\n",
    "* As an example, the titanic data set (more later)\n",
    "\n",
    "|status |age |gender |survived|\n",
    "|-|-|-|-|\n",
    "|crew |adult |female |yes|\n",
    "|first |adult |male |no|\n",
    "|crew |child |female |no|\n",
    "|second |adult |male |yes|\n",
    "\n",
    "* We want to predict/classify survival (i.e., survived is the class)\n",
    "    * Positive instances: instances of the \"main\" class of interest (e.g., yes label)\n",
    "    * Negative instances: all the other instances\n",
    "* $P$ = the # of positive instances in our test set\n",
    "* $N$ = the # of negative instances in our test set\n",
    "* $TP$ = (True Positives) = # of positive instances we classified as positive\n",
    "* $TN$ = (True Negatives) = # of negative instances we classified as negative\n",
    "    * Combined, these are our \"successful\" predictions\n",
    "* $FP$ (False Positives) = # of negative instances we classified as positive\n",
    "* $FN$ (False Negatives) = # of positive instances we classified as negative\n",
    "    * Combined, these are our \"failed\" predictions\n",
    "\n",
    "A generalized \"confusion matrix\" for (binary) classification\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U4-Supervised-Learning/master/figures/binary_confusion_matrix.png\" width=\"400\">\n",
    "\n",
    "## Metrics\n",
    "### Accuracy\n",
    "Accuracy: % of test instances correctly classified by the classifier\n",
    "$$Accuracy = \\frac{TP + TN}{P + N} = \\frac{TP + TN}{TP + FP + TN + FN}$$\n",
    "* Sometimes called \"recognition rate\"\n",
    "* Referred to as \"predictive accuracy\" in the textbook\n",
    "* Warning: can be skewed if unbalanced distribution of class labels\n",
    "    * e.g., lots of negative cases that are easily detected (e.g. 99% accuracy when 99% of the dataset is the negative class)\n",
    "    * shadows performance on positive cases\n",
    "    \n",
    "### Lab Task 2\n",
    "What is the accuracy for the following binary classification confusion matrix?\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U4-Supervised-Learning/master/figures/accuracy_exercise.png\" width=\"300\">\n",
    "\n",
    "[//]: # ($$Accuracy = \\frac{TP + TN}{P + N} = \\frac{18 + 8}{40} = 65\\%$$)\n",
    "    \n",
    "### Accuracy for Multi-class Classification\n",
    "Q: How do we adopt/apply accuracy to MPG data set (lots of classes)?\n",
    "* This is called \"multi-class classification\" (vs \"binary\" classification)\n",
    "\n",
    "Multi-Class Accuracy Example: Assume labels $L = \\{a, b, c\\}$ and $R = \\#$ of total instances\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U4-Supervised-Learning/master/figures/multi_confusion_matrix.png\" width=\"400\">\n",
    "\n",
    "* One approach:\n",
    "    * Number of correctly classified divided by total number of instances $Accuracy = \\frac{TP + TN}{P+N} = \\frac{R_a^a + R_b^b + R_c^c}{R}$\n",
    "    * Easily skewed by certain classes\n",
    "* Another approach:\n",
    "    * Average accuracy per class label\n",
    "    * Basically one binary confusion matrix per label (then average of these)\n",
    "    * If $L$ is the number of labels: $$\\frac{\\sum_{i=1}^{L}\\frac{TP_i+TN_i}{P_i+N_i}}{L}$$\n",
    "    * Have to be careful of empty classes in the test set (don't include in $L$)\n",
    "    \n",
    "To compute the accuracy of label \"a\": $$Accuracy_a = \\frac{TP_a + TN_a}{P_a+N_a}\\\\= \\frac{R_a^a + (R_b^b + R_b^c +R_c^b+R_c^c)}{R_a + (R_b + R_c)}\\\\=\\frac{R - (FN_a +FP_a)}{R}\\\\=\\frac{R - (R_a^b+R_a^c+R_b^a+R_c^a)}{R}$$\n",
    "\n",
    "We could do this for each label, then average the results\n",
    "\n",
    "### Lab Task 3\n",
    "What is the accuracy for the following multi-class classification confusion matrix?\n",
    "\n",
    "Coffee acidity labels: dry, sharp, moderate, or dull\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U4-Supervised-Learning/master/figures/multi_class_accuracy_exercise.png\" width=\"400\">\n",
    "\n",
    "1. 1st Approach (percent correctly classified):\n",
    "1. 2nd Approach (average accuracy per label). First let's do the label dry:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U4-Supervised-Learning/master/figures/multi_class_exercise_matrix.png\" width=\"400\">\n",
    "\n",
    "Then finish the approach for the remaining labels:\n",
    "\n",
    "### Error Rate\n",
    "Error Rate: 1 - accuracy\n",
    "$$ErrorRate = \\frac{FP + FN}{P + N}$$\n",
    "* Has same issues as accuracy (unbalanced labels)\n",
    "* For multi-class classification, can take the average error rate per class\n",
    "\n",
    "### Precision\n",
    "Precision: measure of \"exactness\"\n",
    "* % of instances labeled as positive that are positive\n",
    "* When a classifier predicts positive, it is correct $precision$ percent of the time\n",
    "* A classifier with no false positives has a precision of 1\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "### Recall\n",
    "Recall (AKA sensitivity): measure of \"completeness\"\n",
    "* % of positive instances that are labeled as positive (e.g. labeled correctly)\n",
    "* A classifier correctly classifies $recall$ percent of all positive cases\n",
    "* A classifier with no false negatives has a precision of 1\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Note: There is a trade-off between precision and recall. For a balanced class dataset, a model that predicts mostly positive examples will have a high recall and a low precision.\n",
    "\n",
    "Q: How can we get a high recall score?\n",
    "* Label everything as positive\n",
    "* Note that precision helps keep us honest\n",
    "\n",
    "Q: What about for precision?\n",
    "* Be conservative with our positive labels\n",
    "\n",
    "### F-Measure \n",
    "F-Measure (AKA F1 score): combine the two via the harmonic mean of precision and recall:\n",
    "$$F = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}$$\n",
    "* Summarizes a classifier in a single number (however, it is best practice to still investigate precision and recall, as well as other evaluation metrics)\n",
    "* Alternatively, we can weight precision:\n",
    "$$F_\\beta = \\frac{(1+\\beta^2) \\times Precision \\times Recall}{\\beta^2 \\times Precision + Recall}$$\n",
    "* Helps deal with class imbalance problem\n",
    "\n",
    "### Lab Task 4\n",
    "What is the precision, recall, and F-measure for the win-lose (binary) example?\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U4-Supervised-Learning/master/figures/accuracy_exercise.png\" width=\"300\">\n",
    "\n",
    "### Precision, Recall, and F-Measure for Multi-class Classification\n",
    "* \"Macro\" averaging $M$ (compute each label's precision (or recall) and average over number of labels)\n",
    "$$Precision_M = \\frac{\\sum_{i=1}^{L}\\frac{TP_i}{TP_i + FP_i}}{L}$$\n",
    "\n",
    "$$Recall_M = \\frac{\\sum_{i=1}^{L}\\frac{TP_i}{TP_i + FN_i}}{L}$$\n",
    "\n",
    "$$F_M = \\frac{2 \\times Precision_M \\times Recall_M}{Precision_M + Recall_M}$$\n",
    "\n",
    "* \"Micro\" average $\\mu$ (compute TP and FP (or FN) over all the labels to compute precision (or recall)\n",
    "$$Precision_\\mu = \\frac{\\sum_{i=1}^{L} TP_i}{\\sum_{i=1}^{L} (TP_i + FP_i)}$$\n",
    "\n",
    "$$Recall_\\mu = \\frac{\\sum_{i=1}^{L} TP_i}{\\sum_{i=1}^{L}(TP_i + FN_i)}$$\n",
    "\n",
    "$$F_\\mu = \\frac{2 \\times Precision_\\mu \\times Recall_\\mu}{Precision_\\mu + Recall_\\mu}$$\n",
    "\n",
    "* Macro-averaging treats all classes equally\n",
    "* Micro-averaging favors bigger classes\n",
    "\n",
    "### Lab Task 5\n",
    "What is the precision, recall, and F-measure for the coffee acidity (multi-class) example?\n",
    "1. Using the \"Macro\" approach\n",
    "2. Using the \"Micro\" approach\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U4-Supervised-Learning/master/figures/multi_class_accuracy_exercise.png\" width=\"400\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
