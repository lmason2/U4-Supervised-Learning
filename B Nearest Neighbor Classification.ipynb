{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Nearest Neighbor Classification\n",
    "What are our learning objectives for this lesson?\n",
    "* Learn about the kNN classification algorithm\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s)\n",
    "1. Finish our trace of kNN to make a class prediction for the unseen instance (3, 7)\n",
    "1. Create a new directory in your container called ClassificationFun and add a main.py\n",
    "    * Write a `compute_euclidan_distance(v1, v2)` function that accepts two feature vectors and returns the Euclidean distance between them.\n",
    "    * Consider the following labeled dataset, represented as the lists below. Assume the two attributes have been normalized by scaling to a value between 0 and 10.\n",
    "    * Copy and paste the following lists. Then, compute the distance between each train instance and the test instance. What are the three closest neighbors of the following instance and how would you classify it?\n",
    "        * Note: together we will test our implementation against SciPy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html)\n",
    "\n",
    "```python\n",
    "header = [\"att1\", \"att2\"]\n",
    "train = [\n",
    "    [3, 2],\n",
    "    [6, 6],\n",
    "    [4, 1],\n",
    "    [4, 4],\n",
    "    [1, 2],\n",
    "    [2, 0],\n",
    "    [0, 3],\n",
    "    [1, 6]\n",
    "]\n",
    "train_labels = [\"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"yes\"]\n",
    "test = [2, 3]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today \n",
    "* Announcements\n",
    "    * No RQ this week\n",
    "    * PA3 is due tonight. Questions?\n",
    "    * PA4 is posted. Please read through it this weekend and try to complete part 1 steps 1 and 2. We will finish what you need for the rest of PA4 on Tuesday and I'd be happy to take questions.\n",
    "* kNN and how to divide a dataset into training/testing sets\n",
    "* IQ4 last ~15 mins of class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Classification\n",
    "Nearest neighbor classification is typically used when attributes are continuous\n",
    "* Can be modified for categorical data ...\n",
    "\n",
    "Basic approach:\n",
    "* Given an instance $i$ with $n - 1$ attributes (where the $n^{th}$ is class label)\n",
    "* Find the \"closest\" instance $j$ to $i$ on the $n - 1$ attributes\n",
    "* Use $j$'s class as the prediction for $i$\n",
    "\n",
    "Example from book:\n",
    "Given the data set\n",
    "\n",
    "|a |b |c |d |e |f |Class|\n",
    "|-|-|-|-|-|-|-|\n",
    "|yes |no |no |6.4 |8.3 |low |negative|\n",
    "|yes |yes |yes |18.2 |4.7 |high |positive|\n",
    "\n",
    "What should this instance's classification be?\n",
    "\n",
    "|yes |no |no |6.6 |8.0 |low |???|\n",
    "|-|-|-|-|-|-|-|\n",
    "\n",
    "Usually it isn't this easy!\n",
    "\n",
    "## k Nearest Neighbors\n",
    "Find the k nearest neighbors ...\n",
    "* Usually find the k closest neighbors (instead of just closest)\n",
    "* Then pick classification from among the top k\n",
    "\n",
    "What are good values for the number of neighbors k?\n",
    "* Often done experimentally (with a test set)\n",
    "* Start with k = 1, determine \"error rate\" (more later)\n",
    "* Repeat incrementing k\n",
    "* Pick k with smallest (minimum) error rate\n",
    "     * Often, larger the data (training) set, the larger the k\n",
    "\n",
    "Lets say we found the k nearest neighbors for an instance ...\n",
    "\n",
    "Q: What are ways we could pick the class?\n",
    "* Most frequent occurring class\n",
    "* Weighted \"average\" (based on the relative closest of the k)\n",
    "\n",
    "Note: can use k-NN for regression if, e.g., return the mean of the label values\n",
    "\n",
    "## Distance Functions\n",
    "k-NN works by calculating distances between instances\n",
    "* Many possible ways to do this ... generalized through \"distance measures\"\n",
    "* For two points $x$ and $y$, the distance between them is given by $dist(x,y)$\n",
    "\n",
    "Properties of distance measures (metrics)\n",
    "1. $\\forall x$, $dist(x,x) = 0$\n",
    "    * The distance of any point x from itself is zero\n",
    "2. $\\forall xy$, $dist(x,y) = dist(y,x)$\n",
    "    * Symmetry\n",
    "3. $\\forall xyz$, $dist(x,y) \\leq dist(x,z) + dist(z,y)$\n",
    "    * Triangle equality\n",
    "    * \"Shortest distance between any two points is a straight line\"\n",
    "\n",
    "Euclidean Distance is most often used: Given an instance, treat it as a \"vector\" in n space\n",
    "* Use Pythagoras' Theorem to find distance between them\n",
    "\n",
    "For example:\n",
    "* Given two points (i.e., rows) with $n = 2$: $(x_1, y_1)$ and $(x_2, y_2)$\n",
    "* The length (i.e., distance) of the straight line joining the points is:\n",
    "\n",
    "$$\\sqrt{(x_1 - x_2)^{2} + (y_1 - y_2)^{2}}$$\n",
    "\n",
    "* Euclidean $n$-space\n",
    "    * For rows A = $(a_1, a_2,..., a_n)$ and $B = (b_1, b_2,..., b_n)$ with $n$ attributes\n",
    "$$\\sqrt{(a_1 - b_1)^{2} + (a_2 - b_2)^{2} +...+ (a_n - b_n)^{2}}$$\n",
    "        * Which is:\n",
    "$$\\sqrt{\\sum_{i=1}^{n}(a_i - b_i)^{2}}$$\n",
    "\n",
    "Other examples are described in the book (e.g., Manhattan \"city block\" distance)\n",
    "\n",
    "Q: Do you see any possible issues with Euclidean distance?\n",
    "* Larger values tend to dominate smaller ones\n",
    "* Can degrade into a few attributes driving the distances\n",
    "    * e.g., [Mileage=18,457, Doors=2, Age=12]\n",
    "    \n",
    "## Normalization\n",
    "One solution is to scale all values between 0 and 1 (\"min-max\" normalization)\n",
    "* Use the formula: `(x - min(xs)) / ((max(xs) - min(xs)) * 1.0)`\n",
    "\n",
    "Q: How can we deal with categorical values?\n",
    "* $dist(v_1, v_2) = 0$ if values $v_1 = v_2$\n",
    "    * Same values\n",
    "* For nominal values, $dist(v_1, v_2) = 1$ if $v_1 \\neq v_2$\n",
    "    * Different values\n",
    "* For ordinal values, assign to 1 or use the \"distance\"\n",
    "\n",
    "Q: What do we do about missing values?\n",
    "* Don't have missing values\n",
    "    * Clean the data first\n",
    "* Be conservative\n",
    "    * Assuming normalized\n",
    "    * If only one value missing:\n",
    "        * Assume the maximum possible distance\n",
    "        * If nominal use the maximum distance (i.e., 1)\n",
    "        * If ordinal use either 1 or furthest distance from known value\n",
    "    * otherwise, if both values missing, use the maximum distance (e.g., 1)\n",
    "\n",
    "Distance-based metrics imply equal weighting of attributes\n",
    "* Sometimes can perform better with attribute weights (i.e., some attributes worth more)\n",
    "* \"Feature reduction\" (not using certain attributes) can also help with redundant or \"noisy\" attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic k-NN Algorithm\n",
    "```\n",
    "Input: list of rows, no of atts (n where nth is label), instance to classify, k\n",
    "def kNN_classifier(training_set, n, instance, k):\n",
    "    row_distances = []\n",
    "    for row in training_set:\n",
    "        d = distance(row, instance, n - 1)\n",
    "        row_distances.append([d, row])\n",
    "    top_k_rows = get_top_k(row_distances, k)\n",
    "    label = select_class_label(top_k_rows)\n",
    "    return label\n",
    "```\n",
    "\n",
    "## kNN Example 1\n",
    "Example adapted from [this kNN example](https://people.revoledu.com/kardi/tutorial/KNN/KNN_Numerical-example.html)\n",
    "\n",
    "Suppose we have the following dataset that has two attributes (acid durability and strength) and a class attribute (whether a special paper tissue is good or not):\n",
    "\n",
    "|Acid durability (seconds)|Strength (kg/square meter)|Classification|\n",
    "|-|-|-|\n",
    "|7|7|Bad|\n",
    "|7|4|Bad|\n",
    "|3|4|Good|\n",
    "|1|4| Good|\n",
    "\n",
    "Now the factory produces a new paper tissue with acid durability = 3 seconds and strength = 7 kg/square meter. Can we predict what the classification of this new tissue is? \n",
    "\n",
    "Use kNN with $k$ = 3. Steps:\n",
    "1. Normalize\n",
    "1. Compute distance of each training instance to the test instance\n",
    "1. Determine the majority classification of the $k$ closest instances... this is your prediction for the test instance\n",
    "\n",
    "    \n",
    "### Some Notes\n",
    "Q: What happens if there are ties in the top-k distances (get_top_k)? E.g., which are top 3 in: [[.28,$r_1$],[.33,$r_2$],[.33,$r_3$],[.33,$r_4$],[.37,$r_5$]]?\n",
    "* Different options ... e.g.:\n",
    "    * Randomly select from ties\n",
    "    * Do top-k distances (instead of instances)\n",
    "    * Ignore ties (in case above, just use $r_1$ and $r_2$)\n",
    "\n",
    "Nearest doesn't imply near\n",
    "* top-k instances might not be that close to the instance being classified\n",
    "* Especially true as the number of attributes (\"dimensions\") increases\n",
    "    * An example of the \"curse of dimensionality\"\n",
    "* Again, have to use common sense and an understanding of the dataset\n",
    "\n",
    "## kNN Example 2\n",
    "Consider the following labeled dataset, where result denotes class information and the remaining columns have continuous values. Assume the data has been normalized by scaling to a value between 0 and 10.\n",
    "\n",
    "|att1|att2|result|\n",
    "|-|-|-|\n",
    "|3|2|no|\n",
    "|6|6|yes|\n",
    "|4|1|no|\n",
    "|4|4|no|\n",
    "|1|2|yes|\n",
    "|2|0|no|\n",
    "|0|3|yes|\n",
    "|1|6|yes|\n",
    "\n",
    "Assume we have the following instance to classify using $k$-NN for $k$ = 3 with \"majority voting\" to determine the class label from the k closest neighbors. \n",
    "\n",
    "|att1|att2|result|\n",
    "|-|-|-|\n",
    "|2|3|?|\n",
    "\n",
    "1. What are the three closest neighbors of the following instance?\n",
    "1. How would you classify it?\n",
    "\n",
    "### Efficiency issues\n",
    "Q: Is k-NN efficient? Can you find any efficiency issues?\n",
    "* Given a training set with $D$ instances and $k = 1$\n",
    "* $O(D)$ comparisons needed to classify a given instance\n",
    "\n",
    "Q: Can you think of any ways to improve the efficiency?\n",
    "1. Use search trees\n",
    "    * Presort and arrange instances into a search tree\n",
    "    * Can reduce comparisons to $O(log D)$\n",
    "2. Check each training instance in parallel\n",
    "    * Gives $O(1)$ comparisons\n",
    "3. Editing/Pruning\n",
    "    * Filter or remove training tuples that prove useless\n",
    "    * Reduces size of $D$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
